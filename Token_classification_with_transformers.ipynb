{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jACMr6-7mVX0"
      },
      "source": [
        "\n",
        "Tout d'abord, installer les librairies nécessaires: \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "5z8KKeF8qht9"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet transformers datasets evaluate seqeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6G2PhAYmavw"
      },
      "source": [
        "Charger les libraries dans l'environnement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "RUcLQmW4qzwx"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "import zipfile\n",
        "\n",
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "from transformers import pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5upOiFtoS5l"
      },
      "source": [
        "Uploader dans votre environment de travail l'archive zip fournie (corpusCasM2-main.zip) à l'aide de l'explorateur de fichier google colab.\n",
        "\n",
        "Puis décompresser le fichier avec la commande suivante: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "by_1Go0VnsiH"
      },
      "outputs": [],
      "source": [
        "#with zipfile.ZipFile(\"corpusCasM2-main.zip\", \"r\") as archive:\n",
        "        #archive.extractall(\"data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObZWHpjCrEa2"
      },
      "source": [
        "Charger le dataset corpusCasM2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "7f1zOAChq3v3"
      },
      "outputs": [],
      "source": [
        "#corpusCas = load_dataset(\"data/corpusCasM2-main/corpusCasM2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQhWypsIrbta"
      },
      "source": [
        "Afficher un exemple du trainset:\n",
        "\n",
        "id\n",
        ": correspond à l'id du document et le numero de la phrase dans le document \n",
        "\n",
        "tokens\n",
        ": contient le texte de la phrase pre-tokenisé\n",
        "\n",
        "ner_tags\n",
        ": contient les labels au format [BIO](https://en.wikipedia.org/wiki/Inside–outside–beginning_(tagging)). Ils sont affichés ici par leur ID "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using custom data configuration code source-473889fb0da93825\n",
            "Found cached dataset json (C:/Users/mefta/.cache/huggingface/datasets/json/code source-473889fb0da93825/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
            "\n",
            "\n",
            "\n",
            "100%|██████████| 3/3 [00:00<00:00, 59.86it/s]\n"
          ]
        }
      ],
      "source": [
        "corpusCas = datasets.load_dataset('D://reseau de neurone//Projet NLP//code source','test.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "clDNVdiDq4d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['O',\n",
              " 'U-date',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'B-date',\n",
              " 'L-date',\n",
              " 'O',\n",
              " 'B-problem',\n",
              " 'L-problem',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'B-problem',\n",
              " 'L-problem',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'B-problem',\n",
              " 'I-problem',\n",
              " 'I-problem',\n",
              " 'I-problem',\n",
              " 'I-problem',\n",
              " 'L-problem',\n",
              " 'O',\n",
              " 'O',\n",
              " 'B-problem',\n",
              " 'L-problem',\n",
              " 'O']"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpusCas[\"train\"][0]['biluo']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'U-date', 'I-treatment', 'B-date', 'U-treatment', 'B-duration', 'B-frequency', 'I-duration', 'L-test', 'I-test', 'L-problem', 'U-frequency', 'L-frequency', 'L-date', 'U-problem', 'I-problem', 'O', 'U-test', '-', 'I-frequency', 'B-treatment', 'U-duration', 'L-treatment', 'L-duration', 'I-date', 'B-problem', 'B-test'}\n",
            "966\n"
          ]
        }
      ],
      "source": [
        "#my_list = list(set(corpusCas[\"train\"][1]['biluo']))\n",
        "my_list1= corpusCas[\"train\"][:len(corpusCas[\"train\"])]['biluo']\n",
        "len(my_list1)\n",
        "a1=numpy.concatenate( my_list1, axis=0 )\n",
        "a1\n",
        "distinct_train=set(a1)\n",
        "print(distinct_train)\n",
        "len(distinct_train)\n",
        "\n",
        "compte=0\n",
        "liste=corpusCas[\"train\"][:len(corpusCas[\"train\"])][\"biluo\"]\n",
        "tot_mot=numpy.concatenate(liste, axis=0)\n",
        "for i in range(0, len(tot_mot)):\n",
        "    if tot_mot[i]=='-':\n",
        "        compte=compte+1\n",
        "print(compte)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'I-test', 'I-frequency', 'B-problem', 'B-duration', 'B-treatment', 'B-date', '0', 'B-frequency', 'I-duration', 'B-test', 'I-treatment', 'I-date', 'I-problem', 'O'}\n"
          ]
        }
      ],
      "source": [
        "my_list=[]\n",
        "for i in range(0, len(corpusCas[\"train\"])):\n",
        "    my_list.append(corpusCas[\"train\"][i][\"bio\"])\n",
        "my_list\n",
        "distinct=numpy.concatenate(my_list, axis=0)\n",
        "distinct=set(distinct)\n",
        "print(distinct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qc6hPMerhYL"
      },
      "source": [
        "Il est possible de récupérer la liste des labels à partir des features du dataset: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "X3_3WRguq8Sk"
      },
      "outputs": [],
      "source": [
        "# label_list = corpusCas[\"train\"].features[\"bio\"].feature.names\n",
        "# label_list\n",
        "def set_features(tag, labels):\n",
        "    corpusCas[\"train\"].features[tag].feature.names=labels\n",
        "    corpusCas[\"test\"].features[tag].feature.names=labels\n",
        "    corpusCas[\"validation\"].features[tag].feature.names=labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['B-date',\n",
              " 'I-date',\n",
              " 'B-duration',\n",
              " 'I-duration',\n",
              " 'B-problem',\n",
              " 'I-problem',\n",
              " 'B-treatment',\n",
              " 'I-treatment',\n",
              " 'B-test',\n",
              " 'I-test',\n",
              " 'B-frequency',\n",
              " 'I-frequency',\n",
              " 'O']"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels=[\"B-date\",\"I-date\",\"B-duration\",\"I-duration\",\"B-problem\",\"I-problem\",\"B-treatment\",\n",
        "\"I-treatment\",\"B-test\",\"I-test\",\"B-frequency\",\"I-frequency\",\"O\"]\n",
        "set_features(\"bio\", labels)\n",
        "\n",
        "label_list= corpusCas[\"train\"].features[\"bio\"].feature.names\n",
        "label_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_ner_col(X):\n",
        "    ner_tags=[]\n",
        "    for i in range(0, len(corpusCas[X])):\n",
        "        ner=[]\n",
        "        for j in range(0, len(corpusCas[X][i][\"bio\"])):\n",
        "            token=corpusCas[X][i][\"bio\"][j]\n",
        "            if token==\"0\":\n",
        "                token=\"O\"\n",
        "            ner.append(labels.index(token))\n",
        "        ner_tags.append(ner)\n",
        "\n",
        "    print(len(ner_tags))\n",
        "    corpusCas[X]=corpusCas[X].add_column(name=\"ner\", column=ner_tags)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8305\n",
            "2122\n",
            "2545\n"
          ]
        }
      ],
      "source": [
        "add_ner_col(\"train\")\n",
        "add_ner_col(\"validation\")\n",
        "add_ner_col(\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['O']\n",
            "[12]\n"
          ]
        }
      ],
      "source": [
        "print(corpusCas[\"test\"][2][\"bio\"])\n",
        "print(corpusCas[\"test\"][2][\"ner\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc_id</th>\n",
              "      <th>sent_id</th>\n",
              "      <th>token</th>\n",
              "      <th>biluo</th>\n",
              "      <th>bio</th>\n",
              "      <th>pos</th>\n",
              "      <th>ner</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>517</td>\n",
              "      <td>517_015</td>\n",
              "      <td>[\\n\\n, L’, exploration, per, -, opératoire, a, retrouvé, un, utérus, didelphe, (, Figure, 4, ), ,, des, ovaires, hypoplasiques, et, un, rein, droit, hypoplasique, en, ectopie, pelvienne, .]</td>\n",
              "      <td>[O, O, B-test, I-test, I-test, L-test, O, O, O, B-problem, L-problem, O, O, O, O, O, O, B-problem, L-problem, O, O, B-problem, I-problem, L-problem, O, B-test, L-test, O]</td>\n",
              "      <td>[O, O, B-test, I-test, I-test, I-test, O, O, O, B-problem, I-problem, O, O, O, O, O, O, B-problem, I-problem, O, O, B-problem, I-problem, I-problem, O, B-test, I-test, O]</td>\n",
              "      <td>[SPACE, PROPN, NOUN, ADP, PROPN, NOUN, AUX, VERB, DET, NOUN, ADJ, PUNCT, PROPN, NUM, PUNCT, PUNCT, DET, NOUN, ADJ, CCONJ, DET, NOUN, ADJ, ADJ, ADP, NOUN, ADJ, PUNCT]</td>\n",
              "      <td>[12, 12, 8, 9, 9, 9, 12, 12, 12, 4, 5, 12, 12, 12, 12, 12, 12, 4, 5, 12, 12, 4, 5, 5, 12, 8, 9, 12]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>618</td>\n",
              "      <td>618_020</td>\n",
              "      <td>[\\n\\n]</td>\n",
              "      <td>[O]</td>\n",
              "      <td>[O]</td>\n",
              "      <td>[SPACE]</td>\n",
              "      <td>[12]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import ClassLabel, Sequence\n",
        "import random\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "def show_random_elements(dataset, num_examples=2):\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "    \n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    for column, typ in dataset.features.items():\n",
        "        if isinstance(typ, ClassLabel):\n",
        "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
        "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
        "    display(HTML(df.to_html()))\n",
        "show_random_elements(corpusCas[\"train\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go0HZCNJ_3z2"
      },
      "source": [
        "Télécharger le tokenizer du modèle choisi (ici \"distilbert-base-uncased\"): "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "OSTpjjyjrvoy"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at C:\\Users\\mefta/.cache\\huggingface\\hub\\models--camembert-base\\snapshots\\3f452b6e5a89b0e6c828c9bba2642bc577086eae\\config.json\n",
            "Model config CamembertConfig {\n",
            "  \"_name_or_path\": \"camembert-base\",\n",
            "  \"architectures\": [\n",
            "    \"CamembertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 5,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 6,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"camembert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32005\n",
            "}\n",
            "\n",
            "loading file sentencepiece.bpe.model from cache at C:\\Users\\mefta/.cache\\huggingface\\hub\\models--camembert-base\\snapshots\\3f452b6e5a89b0e6c828c9bba2642bc577086eae\\sentencepiece.bpe.model\n",
            "loading file tokenizer.json from cache at C:\\Users\\mefta/.cache\\huggingface\\hub\\models--camembert-base\\snapshots\\3f452b6e5a89b0e6c828c9bba2642bc577086eae\\tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at C:\\Users\\mefta/.cache\\huggingface\\hub\\models--camembert-base\\snapshots\\3f452b6e5a89b0e6c828c9bba2642bc577086eae\\config.json\n",
            "Model config CamembertConfig {\n",
            "  \"_name_or_path\": \"camembert-base\",\n",
            "  \"architectures\": [\n",
            "    \"CamembertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 5,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 6,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"camembert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32005\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"camembert-base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kww0Eq74ov5Q"
      },
      "source": [
        "Voici un exemple d'output du tokenizer: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "KAHbDylhE2R3"
      },
      "outputs": [],
      "source": [
        "# example = corpusCas[\"train\"][0]\n",
        "# tokenized_input = tokenizer(example[\"token\"], is_split_into_words=True)\n",
        "# tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "# tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['En', '1988', ',', 'F.G.', ',', 'un', 'homme', 'de', '58', 'ans', ',', 'tabagique', 'ancien', ',', 'a', 'présenté', 'une', 'hématurie', 'totale', 'en', 'rapport', 'avec', 'une', 'tumeur', 'vésicale', 'unique', ',', 'paraméatique', 'gauche', ',', 'sans', 'anomalie', 'urographique', '.']\n",
            "\n",
            "{'input_ids': [5, 107, 10500, 21, 7, 358, 9, 546, 9, 21, 7, 23, 421, 8, 6928, 134, 21, 7, 19334, 55, 10406, 1808, 21, 7, 33, 2186, 28, 21, 22488, 16486, 2273, 22, 459, 42, 28, 16775, 8890, 10, 6533, 35, 931, 21, 7, 37, 55, 2037, 55, 1603, 953, 21, 7, 112, 15841, 21, 297, 11136, 21, 9, 6], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "\n",
            "['<s>', '▁En', '▁1988', '▁', ',', '▁F', '.', 'G', '.', '▁', ',', '▁un', '▁homme', '▁de', '▁58', '▁ans', '▁', ',', '▁tab', 'a', 'gique', '▁ancien', '▁', ',', '▁a', '▁présenté', '▁une', '▁', 'hémat', 'urie', '▁totale', '▁en', '▁rapport', '▁avec', '▁une', '▁tumeur', '▁vé', 's', 'ical', 'e', '▁unique', '▁', ',', '▁par', 'a', 'mé', 'a', 'tique', '▁gauche', '▁', ',', '▁sans', '▁anomalie', '▁', 'ur', 'ographique', '▁', '.', '</s>']\n"
          ]
        }
      ],
      "source": [
        "example = corpusCas[\"train\"][0]\n",
        "print(example[\"token\"])\n",
        "print()\n",
        "tokenized_input = tokenizer(example[\"token\"], is_split_into_words=True)\n",
        "print(tokenized_input)\n",
        "print()\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEe-qIlMo0iM"
      },
      "source": [
        "Vous pouvez constater que le texte tokenisé comprend plus de tokens que de mots initialement présents. Or les labels n'existent que pour un mot. Il faut donc réaligner les tokens et les labels en utilisant la fonction suivante: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "X4n5uQhQAT8V"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"token\"], truncation=True, is_split_into_words=True)\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[f\"ner\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "ljjRbG3yA2yF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at C:\\Users\\mefta\\.cache\\huggingface\\datasets\\json\\code source-473889fb0da93825\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-41dd8592b9351ad2.arrow\n",
            "Loading cached processed dataset at C:\\Users\\mefta\\.cache\\huggingface\\datasets\\json\\code source-473889fb0da93825\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-e008d0134dfba622.arrow\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  1%|▏         | 21/1560 [7:31:07<551:00:56, 1288.93s/it]\n",
            "  0%|          | 0/390 [7:25:23<?, ?it/s]\n",
            "  0%|          | 0/780 [30:51<?, ?it/s]\n",
            "\n",
            "\n",
            "\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.27ba/s]\n"
          ]
        }
      ],
      "source": [
        "tokenized_corpus = corpusCas.map(tokenize_and_align_labels, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foTbL8F9qIRY"
      },
      "source": [
        "On obtient bien l'effet escompté: \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "Dktf4_-bA7pl"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'doc_id': 743,\n",
              " 'sent_id': '743_001',\n",
              " 'token': ['\\n\\n',\n",
              "  'L´échographie',\n",
              "  'rénale',\n",
              "  'et',\n",
              "  'vésicoprostatique',\n",
              "  'a',\n",
              "  'objectivé',\n",
              "  'une',\n",
              "  'urétérohydronéphrose',\n",
              "  'bilatérale',\n",
              "  'et',\n",
              "  'la',\n",
              "  'présence',\n",
              "  'd’',\n",
              "  'un',\n",
              "  'résidu',\n",
              "  'post',\n",
              "  'mictionnel',\n",
              "  'significatif',\n",
              "  '.'],\n",
              " 'biluo': ['O',\n",
              "  'B-test',\n",
              "  'I-test',\n",
              "  'I-test',\n",
              "  'L-test',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-problem',\n",
              "  'L-problem',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-problem',\n",
              "  'I-problem',\n",
              "  'L-problem',\n",
              "  'B-problem',\n",
              "  'L-problem',\n",
              "  'O'],\n",
              " 'bio': ['O',\n",
              "  'B-test',\n",
              "  'I-test',\n",
              "  'I-test',\n",
              "  'I-test',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-problem',\n",
              "  'I-problem',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-problem',\n",
              "  'I-problem',\n",
              "  'I-problem',\n",
              "  'B-problem',\n",
              "  'I-problem',\n",
              "  'O'],\n",
              " 'pos': ['SPACE',\n",
              "  'PROPN',\n",
              "  'ADJ',\n",
              "  'CCONJ',\n",
              "  'ADJ',\n",
              "  'AUX',\n",
              "  'VERB',\n",
              "  'DET',\n",
              "  'NOUN',\n",
              "  'ADJ',\n",
              "  'CCONJ',\n",
              "  'DET',\n",
              "  'NOUN',\n",
              "  'ADP',\n",
              "  'DET',\n",
              "  'NOUN',\n",
              "  'ADJ',\n",
              "  'ADJ',\n",
              "  'ADJ',\n",
              "  'PUNCT'],\n",
              " 'ner': [12, 8, 9, 9, 9, 12, 12, 12, 4, 5, 12, 12, 12, 12, 4, 5, 5, 4, 5, 12],\n",
              " 'input_ids': [5,\n",
              "  71,\n",
              "  21,\n",
              "  3,\n",
              "  12182,\n",
              "  4745,\n",
              "  20638,\n",
              "  14,\n",
              "  8890,\n",
              "  10,\n",
              "  6410,\n",
              "  2156,\n",
              "  22592,\n",
              "  33,\n",
              "  21,\n",
              "  23342,\n",
              "  3089,\n",
              "  141,\n",
              "  28,\n",
              "  21,\n",
              "  297,\n",
              "  1146,\n",
              "  1259,\n",
              "  21603,\n",
              "  1079,\n",
              "  20833,\n",
              "  10,\n",
              "  35,\n",
              "  27820,\n",
              "  14,\n",
              "  13,\n",
              "  922,\n",
              "  18,\n",
              "  12,\n",
              "  23,\n",
              "  18338,\n",
              "  518,\n",
              "  1471,\n",
              "  1212,\n",
              "  5616,\n",
              "  2378,\n",
              "  13071,\n",
              "  21,\n",
              "  9,\n",
              "  6],\n",
              " 'attention_mask': [1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1],\n",
              " 'labels': [-100,\n",
              "  8,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  9,\n",
              "  9,\n",
              "  9,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  12,\n",
              "  12,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  12,\n",
              "  4,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  5,\n",
              "  12,\n",
              "  12,\n",
              "  12,\n",
              "  12,\n",
              "  -100,\n",
              "  4,\n",
              "  5,\n",
              "  -100,\n",
              "  5,\n",
              "  4,\n",
              "  -100,\n",
              "  -100,\n",
              "  5,\n",
              "  12,\n",
              "  -100,\n",
              "  -100]}"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_corpus[\"test\"][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2R8WQI2iqQWu"
      },
      "source": [
        "On crée ensuite un [DataCollator](https://huggingface.co/docs/transformers/main_classes/data_collator) qui va servir à créer les batchs en entrée du modèle. On utilise un DataCollator spécifique à la tache "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "PhlaD7ebA-10"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7B7G4nlq3ut"
      },
      "source": [
        "Ensuite on crée une petite fonction qui va être chargée de faire l'évaluation du modèle: ici on se sert de la métrique [seqeval](https://huggingface.co/spaces/evaluate-metric/seqeval).  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "K_Zb3pkNEP7j"
      },
      "outputs": [],
      "source": [
        "\n",
        "seqeval = evaluate.load(\"seqeval\")\n",
        "seq_labels = [label_list[i] for i in example[f\"ner\"]]\n",
        "\n",
        "\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuj6JbLfrTa5"
      },
      "source": [
        "Créons à présent les dictionnaires qui permettent de lier les labels avec leurs IDs respectifs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "hAs_FuZ0EdGz"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'B-date': 0,\n",
              " 'I-date': 1,\n",
              " 'B-duration': 2,\n",
              " 'I-duration': 3,\n",
              " 'B-problem': 4,\n",
              " 'I-problem': 5,\n",
              " 'B-treatment': 6,\n",
              " 'I-treatment': 7,\n",
              " 'B-test': 8,\n",
              " 'I-test': 9,\n",
              " 'B-frequency': 10,\n",
              " 'I-frequency': 11,\n",
              " 'O': 12}"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "id2label = {i:label for i,label in enumerate(label_list)}\n",
        "label2id = {v:k for k,v in id2label.items()}\n",
        "label2id\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvySE6slrogF"
      },
      "source": [
        "Nous pouvons à présent instancier le modèle que nous voulons fine-tuner: \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "Xa408u4lFFmZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file config.json from cache at C:\\Users\\mefta/.cache\\huggingface\\hub\\models--camembert-base\\snapshots\\3f452b6e5a89b0e6c828c9bba2642bc577086eae\\config.json\n",
            "Model config CamembertConfig {\n",
            "  \"_name_or_path\": \"camembert-base\",\n",
            "  \"architectures\": [\n",
            "    \"CamembertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 5,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 6,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"B-date\",\n",
            "    \"1\": \"I-date\",\n",
            "    \"2\": \"B-duration\",\n",
            "    \"3\": \"I-duration\",\n",
            "    \"4\": \"B-problem\",\n",
            "    \"5\": \"I-problem\",\n",
            "    \"6\": \"B-treatment\",\n",
            "    \"7\": \"I-treatment\",\n",
            "    \"8\": \"B-test\",\n",
            "    \"9\": \"I-test\",\n",
            "    \"10\": \"B-frequency\",\n",
            "    \"11\": \"I-frequency\",\n",
            "    \"12\": \"O\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"B-date\": 0,\n",
            "    \"B-duration\": 2,\n",
            "    \"B-frequency\": 10,\n",
            "    \"B-problem\": 4,\n",
            "    \"B-test\": 8,\n",
            "    \"B-treatment\": 6,\n",
            "    \"I-date\": 1,\n",
            "    \"I-duration\": 3,\n",
            "    \"I-frequency\": 11,\n",
            "    \"I-problem\": 5,\n",
            "    \"I-test\": 9,\n",
            "    \"I-treatment\": 7,\n",
            "    \"O\": 12\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"camembert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32005\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at C:\\Users\\mefta/.cache\\huggingface\\hub\\models--camembert-base\\snapshots\\3f452b6e5a89b0e6c828c9bba2642bc577086eae\\pytorch_model.bin\n",
            "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForTokenClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing CamembertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CamembertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of CamembertForTokenClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    \"camembert-base\", num_labels=len(id2label), id2label=id2label, label2id=label2id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ],
      "source": [
        "batch_size=8\n",
        "\n",
        "args = TrainingArguments(\n",
        "    \"camembert-base-finetuned\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_corpus[\"train\"],\n",
        "    eval_dataset=tokenized_corpus[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the training set don't have a corresponding argument in `CamembertForTokenClassification.forward` and have been ignored: sent_id, doc_id, biluo, pos, bio, ner, token. If sent_id, doc_id, biluo, pos, bio, ner, token are not expected by `CamembertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "c:\\Users\\mefta\\miniconda3\\envs\\corpuscas\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 8305\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 3117\n",
            "  Number of trainable parameters = 110041357\n",
            "  0%|          | 0/3117 [00:00<?, ?it/s]You're using a CamembertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 16%|█▌        | 500/3117 [32:05<3:44:36,  5.15s/it]\n",
            "\n",
            "                                                       \n",
            " 16%|█▌        | 500/3117 [32:05<3:44:36,  5.15s/it]Saving model checkpoint to camembert-base-finetuned\\checkpoint-500\n",
            "Configuration saved in camembert-base-finetuned\\checkpoint-500\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.1801, 'learning_rate': 1.679178697465512e-05, 'epoch': 0.48}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in camembert-base-finetuned\\checkpoint-500\\pytorch_model.bin\n",
            "tokenizer config file saved in camembert-base-finetuned\\checkpoint-500\\tokenizer_config.json\n",
            "Special tokens file saved in camembert-base-finetuned\\checkpoint-500\\special_tokens_map.json\n",
            " 32%|███▏      | 1000/3117 [1:52:14<1:59:42,  3.39s/it]  \n",
            "\n",
            "                                                         \n",
            " 32%|███▏      | 1000/3117 [1:52:14<1:59:42,  3.39s/it]Saving model checkpoint to camembert-base-finetuned\\checkpoint-1000\n",
            "Configuration saved in camembert-base-finetuned\\checkpoint-1000\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.6958, 'learning_rate': 1.3583573949310235e-05, 'epoch': 0.96}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in camembert-base-finetuned\\checkpoint-1000\\pytorch_model.bin\n",
            "tokenizer config file saved in camembert-base-finetuned\\checkpoint-1000\\tokenizer_config.json\n",
            "Special tokens file saved in camembert-base-finetuned\\checkpoint-1000\\special_tokens_map.json\n",
            " 33%|███▎      | 1039/3117 [1:54:20<1:36:11,  2.78s/it]The following columns in the evaluation set don't have a corresponding argument in `CamembertForTokenClassification.forward` and have been ignored: sent_id, doc_id, biluo, pos, bio, ner, token. If sent_id, doc_id, biluo, pos, bio, ner, token are not expected by `CamembertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2122\n",
            "  Batch size = 8\n",
            "c:\\Users\\mefta\\miniconda3\\envs\\corpuscas\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "\n",
            "\n",
            "                                                         \n",
            "\u001b[A                                              \n",
            " 33%|███▎      | 1039/3117 [1:58:34<1:36:11,  2.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.5905081033706665, 'eval_precision': 0.5262452107279694, 'eval_recall': 0.616887491578711, 'eval_f1': 0.5679727075364417, 'eval_accuracy': 0.8520063613524789, 'eval_runtime': 253.899, 'eval_samples_per_second': 8.358, 'eval_steps_per_second': 1.048, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 42%|████▏     | 1316/3117 [2:41:07<1:24:17,  2.81s/it]   "
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0olKJj5DsWOq"
      },
      "source": [
        "## TODO\n",
        "\n",
        "Pour simplifier le travail, les **scopes de temporalité ne sont pas pris en compte** dans cette version du dataset. \n",
        "\n",
        "1. Identifier un modèle adapté à la tâche. Ici nous voulons faire de la reconnaissance d'entités nommées (classification de tokens) dans des textes en français. Ce choix devra être argumenté. Dans l'exemple, le modèle choisi n'est pas particulièrement adapté au français. \n",
        "\n",
        "2. Fine-tuner ce modèle sur la tâche. C'est à dire réentrainer le modèle avec les données du corpusCasM2. Vous devrez tester différentes combinaisons d'hyperparamètres afin de trouver les meilleurs. Pour cela, vous mettrez en place une stratégie d'optimisation des hyperparamètres que vous justifierez.\n",
        "\n",
        "3. Evaluer de manière adaptée et argumentée les résultats obtenus.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "corpuscas",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "83254ae805c6f06370d9d95d22a28c5f73e1c0057c1c74d3ca0bd53810118643"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
